[{"count":1,"createdAt":1746427627,"description":"潮汐收藏使用文档","folderId":"my","icon":"https://help.tidemark.cc/icon.png","id":"8wzCeHk2FUAd","name":"欢迎使用|潮汐收藏","topUpTime":0,"uflag":0,"updatedAt":1746430029,"url":"https://help.tidemark.cc/"},{"count":3,"createdAt":1746453610,"description":"文章浏览阅读4.1k次，点赞22次，收藏32次。本教程将指导你在 Windows 系统上安装 Docker 和 Ollama，下载 DeepSeek-V3 模型，并通过 Open WebUI 提供一个用户友好的 Web 界面来与模型交互。_open-webui docker镜像","folderId":"my","icon":"https://g.csdnimg.cn/static/logo/favicon32.ico","id":"ZM7onHjX4z84","name":"DeepSeek教程：在 Windows 下安装 Docker、Ollama，并通过 Open WebUI 部署本地 DeepSeek_open-webui docker镜像-CSDN博客","topUpTime":0,"uflag":0,"updatedAt":1747492542,"url":"https://blog.csdn.net/2301_81940605/article/details/145488304"},{"count":0,"createdAt":1746619356,"description":"此应用程序为您的英特尔硬件提供驱动程序和软件更新。","folderId":"my","icon":"https://www.intel.cn/etc.clientlibs/settings/wcm/designs/intel/default/resources/favicon.ico","id":"hxTInOxiJX1j","name":"英特尔® 驱动程序和支持助理","topUpTime":0,"updatedAt":1746619356,"url":"https://www.intel.cn/content/www/cn/zh/support/intel-driver-support-assistant.html"},{"count":2,"createdAt":1746629688,"description":"概述当前AI在企业应用时，如何让模型输出符合企业对应业务的知识并输出稳定，是企业应用的关键，当前主要有三种方式， 长文本、知识库和微调。长文本就是我们经常说的提示词，知识库就是构建本地知识库，根据问题…","folderId":"my","icon":"https://static.zhihu.com/heifetz/favicon.ico","id":"df1rXkjM6A_S","name":"open-webui+RAG搭建本地知识库，初步构建企业级应用 - 知乎","topUpTime":0,"uflag":0,"updatedAt":1747485707,"url":"https://zhuanlan.zhihu.com/p/25771946889"},{"count":1,"createdAt":1746629988,"description":"DeepSeek + Dify ：零成本搭建企业级本地私有化知识库保姆级喂饭教程最近，DeepSeek大火，想必大家都有所耳闻，各路媒体从各个方面报道了DeepSeek这家神秘的公司的各方面消息，这家低调的技术公司用一组硬核数据回…","folderId":"my","icon":"https://static.zhihu.com/heifetz/favicon.ico","id":"USIHD7Mx-tCe","name":"DeepSeek + Dify ：零成本搭建企业级本地私有化知识库保姆级教程 - 知乎","topUpTime":0,"uflag":0,"updatedAt":1747493445,"url":"https://zhuanlan.zhihu.com/p/20619350390"},{"count":0,"createdAt":1747147554,"description":"摘要基于Intel Core Ultra平台实测，通过IPEX-LLM项目实现Ollama大语言模型在Windows 11系统下的免安装部署，利用Meteor Lake架构的NPU+GPU异构计算单元，实测模型推理速度可提升约100%（完整测试数据见文末表格）…","folderId":"my","icon":"https://static.zhihu.com/heifetz/favicon.ico","id":"J4GNaIN5VW6X","name":"让Ollama使用Intel Ultra/Arc GPU加速模型推理 - 知乎","topUpTime":0,"updatedAt":1747147554,"url":"https://zhuanlan.zhihu.com/p/29653307917"},{"count":1,"createdAt":1747147559,"description":"Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain","folderId":"my","icon":"https://github.githubassets.com/favicons/favicon.svg","id":"pkbRnyMz48II","name":"Release 2.3.0 nightly build · ipex-llm/ipex-llm","topUpTime":0,"updatedAt":1747493510,"url":"https://github.com/ipex-llm/ipex-llm/releases/tag/v2.3.0-nightly"},{"count":0,"createdAt":1747485639,"description":"","folderId":"my","icon":"https://www.docker.com/favicon.ico","id":"2aES-lAai_BY","name":"Docker Desktop redirect | Docker","topUpTime":0,"updatedAt":1747485639,"url":"https://app.docker.com/auth/desktop/redirect?code=a28dvX-qNeuGGhAPC62PvjGAcrPXLsXNuWU4u3x1qrFD3&state=vUqa7znsr3WNcoy37Dv-g980iUOMW06oeJ6t4DnsIcU"},{"count":0,"createdAt":1747485993,"description":"文章浏览阅读1.1w次，点赞32次，收藏70次。使用Ollama + RAGFlow在Windows本地部署，其中RAGFlow通过docker启动_ragflow windows部署","folderId":"my","icon":"https://g.csdnimg.cn/static/logo/favicon32.ico","id":"VcqfNIsD08lw","name":"使用Ollama + RAGFlow在Windows本地部署_ragflow windows部署-CSDN博客","topUpTime":0,"updatedAt":1747485993,"url":"https://blog.csdn.net/m0_46365672/article/details/142868100"},{"count":8,"createdAt":1747496517,"description":"","folderId":"my","icon":"https://testbigdldocshane.readthedocs.io/favicon.ico","id":"5-hsTXggyyPs","name":"在 Intel GPU 上运行带有 IPEX_LLM 的 RAGFlow — IPEX-LLM 最新文档","topUpTime":0,"updatedAt":1748273232,"url":"https://testbigdldocshane.readthedocs.io/en/ragflow-quickstart/doc/LLM/Quickstart/ragflow_quickstart.html#changing-vm-max-map-count"},{"count":0,"createdAt":1747621754,"description":"DeepSeek-R1本地部署配置要求 Github地址：https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file 模型规模最低 GPU 显存推荐 GPU 型号纯 CPU 内存需求适用场景 1.5B 4GB RTX 3050 8GB ","folderId":"my","icon":"https://assets.cnblogs.com/favicon_v3_2.ico","id":"b3kLBJskUnny","name":"DeepSeek R1 + ollama + ragflow 使用 docker 部署（Windows） - 马铃薯1 - 博客园","topUpTime":0,"updatedAt":1747621754,"url":"https://www.cnblogs.com/REN-Murphy/p/18769576"},{"count":0,"createdAt":1747636851,"description":"Docker汉化 Docker中文版 Docker汉化包 DockerDesktop汉化 Docker Windows Docker MAC - asxez/DockerDesktop-CN","folderId":"my","icon":"https://github.githubassets.com/favicons/favicon.svg","id":"_bRk-mF8RfJk","name":"asxez/DockerDesktop-CN: Docker汉化 Docker中文版 Docker汉化包 DockerDesktop汉化 Docker Windows Docker MAC","topUpTime":0,"updatedAt":1747636851,"url":"https://github.com/asxez/DockerDesktop-CN"},{"count":1,"createdAt":1747707273,"description":"Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain","folderId":"my","icon":"https://github.githubassets.com/favicons/favicon.svg","id":"vT6jgT1vvFtV","name":"使用 IPEX-LLM 在 Intel GPU 上运行 llama.cpp","tagIds":[],"topUpTime":0,"uflag":1,"updatedAt":1748093738,"url":"https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llama_cpp_quickstart.zh-CN.md"},{"count":0,"createdAt":1747708343,"description":"Download page for the conda-forge installer","folderId":"my","icon":"https://conda-forge.org/img/favicon.ico","id":"TN5wRbUTI86p","name":"conda-forge | community-driven packaging for conda | conda-forge | community-driven packaging for conda","topUpTime":0,"updatedAt":1747708343,"url":"https://conda-forge.org/download/"},{"count":1,"createdAt":1747722371,"description":"Knowledge Base QA using RAG pipeline on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with IPEX-LLM - Langchain-Chatchat/INSTALL_win_arc.md at ipex-llm · intel-staging/Langchain-Chatchat","folderId":"my","icon":"https://github.githubassets.com/favicons/favicon.svg","id":"uVZYyjiFAfos","name":"使用 Intel Arc A 系列 GPU 的 Windows 设置指南","tagIds":[],"topUpTime":0,"uflag":1,"updatedAt":1748093718,"url":"https://github.com/intel-staging/Langchain-Chatchat/blob/ipex-llm/INSTALL_win_arc.md"},{"count":1,"createdAt":1748065339,"description":"本指南旨在帮助用户在 Windows* 11 和 Ubuntu* 22.04 LTS 上的英特尔硬件平台上使用开放式 WebUI 安装和运行 Ollama。","folderId":"my","icon":"https://www.intel.cn/etc.clientlibs/settings/wcm/designs/intel/default/resources/favicon.ico","id":"01iunvI1Ms9V","name":"Running Ollama with Open WebUI on Intel Hardware Platform","topUpTime":0,"updatedAt":1748093680,"url":"https://www.intel.cn/content/www/cn/zh/content-details/826081/running-ollama-with-open-webui-on-intel-hardware-platform.html"},{"count":0,"createdAt":1749481267,"description":"本文转载于：https://lainbo.com/article/clash-config 仅作为备份查看，更多内容请查看原作者博客。 文章内容会跟随客户端版本更新进行图例和描述更新，最近更新时间：2024-11-18 关于一些客户端作者停止更新 这些客户端停止更新了, 对我们的使用是否有影响呢? ","folderId":"my","icon":"https://photo0919.oss-cn-hangzhou.aliyuncs.com/img/photos.png","id":"St0W5skGTZkS","name":"Clash-Verge-Rev最佳实践","topUpTime":0,"uflag":0,"updatedAt":1749481270,"url":"https://bflome.com/archives/clash-verge-best-practice"},{"count":0,"createdAt":1750169862,"description":"在网盘资源版块中，您可以方便地下载高清电影资源，通过网盘方式快速获取并享受精彩的电影观影体验。 ","folderId":"my","icon":"https://www.4ksj.com/favicon.ico","id":"Cc7io-xHFPcG","name":"【　　　】电视剧 - 网盘资源下载_百度/夸克/115网盘资源大全_4K世界","topUpTime":0,"updatedAt":1750169862,"url":"https://www.4ksj.com/forum-wangpan-t410-1.html"}]